{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "3kQHhAiP2MeR"
   },
   "outputs": [],
   "source": [
    "#@title Customer Data\n",
    "\n",
    "# import os\n",
    "# path = \"/content/obj_train_data/img_plate/\"\n",
    "# for filename in os.listdir(path):\n",
    "#     if (os.stat(path + filename).st_size == 0):\n",
    "#         os.remove(path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XLr8_XNeCmZq"
   },
   "outputs": [],
   "source": [
    "#@title Customer Data\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "# import cv2\n",
    "\n",
    "# TRAIN_IMG = \"/content/train_img/\"\n",
    "# path_lb = \"/content/obj_train_data/img_plate/\"\n",
    "# path_im = \"/content/img_plate/\"\n",
    "# for filename in os.listdir(path_lb):\n",
    "#     shutil.copy(path_im + filename.split('.')[0] + '.jpg', TRAIN_IMG)\n",
    "\n",
    "# print(len(os.listdir(\"/content/train_img/\")))\n",
    "# print(len(os.listdir(\"/content/obj_train_data/img_plate/\")))\n",
    "# # print(len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xN928BzhDlV8"
   },
   "outputs": [],
   "source": [
    "#@title Download yolov5\n",
    "\n",
    "# !git clone https://github.com/ultralytics/yolov5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZKoD71bEBAJ"
   },
   "outputs": [],
   "source": [
    "%cd yolov5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "uL-6QjobEJVE"
   },
   "outputs": [],
   "source": [
    "#@title Install Requirements yolov5\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nbu8HkPUEOBP"
   },
   "outputs": [],
   "source": [
    "#@title Customer Data\n",
    "\n",
    "# !mkdir -p /content/images/train/\n",
    "# !mkdir -p /content/images/valid/\n",
    "# !mkdir -p /content/labels/train/\n",
    "# !mkdir -p /content/labels/valid/\n",
    "# !mkdir -p /content/label/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "yOKPPBv6EilX"
   },
   "outputs": [],
   "source": [
    "#@title Customer Data\n",
    "\n",
    "# TRAIN_IMG = \"/content/images/train/\"\n",
    "# VALID_ING = \"/content/images/valid/\"\n",
    "# TRAIN_LB = \"/content/labels/train/\"\n",
    "# VALID_LB = \"/content/labels/valid/\"\n",
    "\n",
    "# DATA_DIR = \"/content/train_img/\"\n",
    "# LABEL_DIR = \"/content/obj_train_data/img_plate/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "UgQrn4CGEmmc"
   },
   "outputs": [],
   "source": [
    "#@title Customer Data\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "# import cv2\n",
    "\n",
    "# # for filename in os.listdir(DATA_DIR):\n",
    "# #   if filename.endswith(\".txt\"):\n",
    "# #     shutil.move(DATA_DIR + filename, LABEL_DIR)\n",
    "\n",
    "# lst_files = os.listdir(DATA_DIR)\n",
    "# split_ratio = int(len(lst_files) * 0.8)\n",
    "\n",
    "# lst_train = random.sample(lst_files, split_ratio)\n",
    "# lst_valid = list(set(lst_files) - set(lst_train))\n",
    "\n",
    "# for filename in lst_train:\n",
    "#   shutil.move(DATA_DIR + filename, TRAIN_IMG)\n",
    "#   shutil.move(LABEL_DIR + os.path.splitext(filename)[0] + \".txt\", TRAIN_LB)\n",
    "\n",
    "# for filename in lst_valid:\n",
    "#   shutil.move(DATA_DIR + filename, VALID_ING)\n",
    "#   shutil.move(LABEL_DIR + os.path.splitext(filename)[0] + \".txt\", VALID_LB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "DZg_z6p9JYue"
   },
   "outputs": [],
   "source": [
    "#@title Create file dataset.yaml\n",
    "\n",
    "# !touch dataset.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LW8vwOE3FfUM"
   },
   "outputs": [],
   "source": [
    "#@title Writefile dataset.yaml\n",
    "\n",
    "%%writefile dataset.yaml\n",
    "# train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]\n",
    "train: /content/images/train/\n",
    "val: /content/images/valid/\n",
    "\n",
    "# number of classes\n",
    "nc: 1\n",
    "\n",
    "# class names\n",
    "# names: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'K', 'L', 'M', 'N', 'P', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z']\n",
    "names: ['Recognition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4rdJpWCqJVoV"
   },
   "outputs": [],
   "source": [
    "#@title Download Weights\n",
    "\n",
    "!wget https://github.com/ultralytics/yolov5/releases/download/v4.0/yolov5s.pt -O /content/yolov5/yolov5s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTaRn-SwSvP4"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p runs/train/exp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htCOiJcZDFU6"
   },
   "outputs": [],
   "source": [
    "# %cp -rf /content/drive/MyDrive/exp16/weights/ /content/yolov5/runs/train/exp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSq-rxJCSjXJ"
   },
   "outputs": [],
   "source": [
    "# %cp -rf /content/drive/MyDrive/yolov5/runs/train/exp3/ /content/yolov5/runs/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtiJRCxmKtWb"
   },
   "outputs": [],
   "source": [
    "# !python train.py --img 640 --batch 8 --epochs 50 --data dataset.yaml --weight yolov5s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrkz2S9vMvHc"
   },
   "outputs": [],
   "source": [
    "# !python detect.py --save-txt --source data/images/ --weights runs/train/exp/weights/best.pt --conf 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbTCNWeM5nyR"
   },
   "outputs": [],
   "source": [
    "!apt install tesseract-ocr -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8XyePxF5pfD"
   },
   "outputs": [],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "vlqYV7VXDWSb"
   },
   "outputs": [],
   "source": [
    "#@title Recognition Model\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "from termcolor import colored\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "#def recognition()\n",
    "import pytesseract\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import glob\n",
    "#show_img\n",
    "from matplotlib import gridspec\n",
    "\n",
    "class opt01():\n",
    "    source01 = 'data/images/'\n",
    "    weights01 = 'runs/train/exp/weights/best.pt'\n",
    "    img_size = 640\n",
    "    name = 'exp'\n",
    "    project = 'runs/detect'\n",
    "    device = ''\n",
    "    conf_thres = float(0.25)\n",
    "    conf_thres01 = float(0.25)\n",
    "    iou_thres = float(0.45)\n",
    "    save_txt = True\n",
    "    view_img = False\n",
    "    save_conf = False\n",
    "    classes = None\n",
    "    agnostic_nms = False\n",
    "    augment = False\n",
    "    update = False\n",
    "    exist_ok = False\n",
    "    def __init__(self):\n",
    "        opt01.source = source\n",
    "        opt01.weights = weights\n",
    "        opt01.view_img = view_img\n",
    "        opt01.save_txt = save_txt\n",
    "        opt01.img_size = img_size\n",
    "        opt01.name = name\n",
    "        opt01.project = project\n",
    "        opt01.device = device\n",
    "        opt01.conf_thres = conf_thres\n",
    "        opt01.conf_thres01 = conf_thres01\n",
    "        opt01.iou_thres = iou_thres\n",
    "class Recognition():\n",
    "    \n",
    "    def __init__(self, opt01):\n",
    "        self.opt01 = opt01\n",
    "    \n",
    "    def img_plate(self, xywh, im0):     #detect and crop plate\n",
    "        width = im0.shape[1]\n",
    "        height = im0.shape[0]\n",
    "        x = xywh[0]\n",
    "        y = xywh[1]\n",
    "        w = xywh[2]\n",
    "        h = xywh[3]\n",
    "        xmin = int((x - w/2)*width)\n",
    "        ymin = int((y - h/2)*height)\n",
    "        xmax = int(xmin + (w*width))\n",
    "        ymax = int(ymin + (h*height))\n",
    "        \n",
    "        cropped = im0[ymin:ymax, xmin:xmax]\n",
    "\n",
    "        return cropped\n",
    "\n",
    "    def letterbox(self, img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "        # Resize and pad image while meeting stride-multiple constraints\n",
    "        shape = img.shape[:2]  # current shape [height, width]\n",
    "        if isinstance(new_shape, int):\n",
    "            new_shape = (new_shape, new_shape)\n",
    "\n",
    "        # Scale ratio (new / old)\n",
    "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "        if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "            r = min(r, 1.0)\n",
    "\n",
    "        # Compute padding\n",
    "        ratio = r, r  # width, height ratios\n",
    "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "        if auto:  # minimum rectangle\n",
    "            dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "        elif scaleFill:  # stretch\n",
    "            dw, dh = 0.0, 0.0\n",
    "            new_unpad = (new_shape[1], new_shape[0])\n",
    "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "        dw /= 2  # divide padding into 2 sides\n",
    "        dh /= 2\n",
    "\n",
    "        if shape[::-1] != new_unpad:  # resize\n",
    "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "        return img, ratio, (dw, dh)\n",
    "    \n",
    "    \n",
    "\n",
    "    # def getCol(self, lst, col):\n",
    "    #     total = 0\n",
    "    #     index = 0\n",
    "    #     for line in lst:\n",
    "    #         total += line[col]\n",
    "    #     average = total /len(lst)\n",
    "    #     print(average)\n",
    "\n",
    "    #     for idx, line in enumerate(lst):\n",
    "    #         # print(line[idx])\n",
    "    #         if line[idx] >= average:\n",
    "    #             index = idx - 1\n",
    "    #     return index\n",
    "\n",
    "    def recognition(self, img0, save_img=False):\n",
    "        \n",
    "        source, weights, view_img, save_txt, imgsz = opt01.source01, opt01.weights01, opt01.view_img, opt01.save_txt, opt01.img_size\n",
    "        webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "            ('rtsp://', 'rtmp://', 'http://'))\n",
    "\n",
    "        # Directories\n",
    "        save_dir = Path(increment_path(Path(opt01.project) / opt01.name, exist_ok=opt01.exist_ok))  # increment run\n",
    "        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "        # Initialize\n",
    "        set_logging()\n",
    "        device = select_device(opt01.device)\n",
    "        half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "        # Load modelim0s\n",
    "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "        stride = int(model.stride.max())  # model stride\n",
    "        imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "        if half:\n",
    "            model.half()  # to FP16\n",
    "\n",
    "        # Second-stage classifier\n",
    "        classify = False\n",
    "        if classify:\n",
    "            modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "            modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
    "\n",
    "        # Set Dataloader\n",
    "        vid_path, vid_writer = None, None\n",
    "        if webcam:\n",
    "            view_img = check_imshow()\n",
    "            cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "            dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "        else:\n",
    "            save_img = True\n",
    "            dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "        # Get names and colors\n",
    "        names = model.module.names if hasattr(model, 'module') else model.names\n",
    "        colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "\n",
    "        # Run inference\n",
    "        if device.type != 'cpu':\n",
    "            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "        t0 = time.time()\n",
    "        # for path, img, im0s, vid_cap in dataset:\n",
    "        img = self.letterbox(img0, 640, stride=32)[0]\n",
    "        \n",
    "\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        # cv2_imshow(img)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        # cv2_imshow(img)\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        # cv2_imshow(img)\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "        im0s = img0\n",
    "        # print(\"qqqq\")\n",
    "\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img, augment=opt01.augment)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, opt01.conf_thres01, opt01.iou_thres, classes=opt01.classes, agnostic=opt01.agnostic_nms)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Apply Classifier\n",
    "        if classify:\n",
    "            pred = apply_classifier(pred, modelc, img, im0s)\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            \n",
    "            im0 = im0s\n",
    "\n",
    "            # det = det[det[:,3].sort()[1]]\n",
    "            lst = det.tolist()\n",
    "            # print(\"lst\", lst)\n",
    "            sortt = sorted(lst, key = lambda x: x[1], reverse=True)\n",
    "            # index = self.getCol(lst, 2)\n",
    "            index = math.ceil(len(lst)/float(2))\n",
    "            \n",
    "\n",
    "            sortt1 = sorted(sortt[:index], key = lambda x: x[3])\n",
    "            sortt2 = sorted(sortt[index:], key = lambda x: x[3])\n",
    "\n",
    "            det = torch.tensor(sortt1+sortt2)\n",
    "            \n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "                \n",
    "                # Write results\n",
    "                crop_character = []\n",
    "                plate_num = ''\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    \n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    \n",
    "                    x = xywh[0]\n",
    "                    y = xywh[1]\n",
    "                    w = xywh[2]\n",
    "                    h = xywh[3]\n",
    "                    try:\n",
    "                        img_plate = self.img_plate(xywh, im0)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    crop_character.append(img_plate)\n",
    "                    \n",
    "                    text = pytesseract.image_to_string(img_plate, config=r'-l eng -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ --psm 8')\n",
    "                    plate_num += text\n",
    "                                        \n",
    "                    plot_one_box(xyxy, img0, label=None, color=colors[int(cls)], line_thickness=3)\n",
    "                    \n",
    "        \n",
    "        if (crop_character is None):\n",
    "            return None, None\n",
    "        else:\n",
    "            return crop_character, plate_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "NYjJKBqWi7pc"
   },
   "outputs": [],
   "source": [
    "#@title License Plate Model\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "from termcolor import colored\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "\n",
    "#def img_plate()\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "#def rotate()\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import ndimage\n",
    "\n",
    "#def recognition()\n",
    "import pytesseract\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import glob\n",
    "#show_img\n",
    "from matplotlib import gridspec\n",
    "\n",
    "class opt():\n",
    "    # source = 'data/images/'\n",
    "    source = 'data/images/'\n",
    "    weights = 'runs/train/exp3/weights/best.pt'\n",
    "    img_size = 640\n",
    "    name = 'exp'\n",
    "    project = 'runs/detect'\n",
    "    device = ''\n",
    "    conf_thres = float(0.25)\n",
    "    conf_thres01 = float(0.25)\n",
    "    iou_thres = float(0.45)\n",
    "    save_txt = True\n",
    "    view_img = False\n",
    "    save_conf = False\n",
    "    classes = None\n",
    "    agnostic_nms = False\n",
    "    augment = False\n",
    "    update = False\n",
    "    exist_ok = False\n",
    "    def __init__(self):\n",
    "        opt.source = source\n",
    "        opt.weights = weights\n",
    "        opt.view_img = view_img\n",
    "        opt.save_txt = save_txt\n",
    "        opt.img_size = img_size\n",
    "        opt.name = name\n",
    "        opt.project = project\n",
    "        opt.device = device\n",
    "        opt.conf_thres = conf_thres\n",
    "        opt.conf_thres01 = conf_thres01\n",
    "        opt.iou_thres = iou_thres\n",
    "class detectC():\n",
    "    \n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "    \n",
    "    def img_plate(self, xywh, im0):     #detect and crop plate\n",
    "        width = im0.shape[1]\n",
    "        height = im0.shape[0]\n",
    "        x = xywh[0]\n",
    "        y = xywh[1]\n",
    "        w = xywh[2]\n",
    "        h = xywh[3]\n",
    "        xmin = int((x - w/2)*width)\n",
    "        ymin = int((y - h/2)*height)\n",
    "        xmax = int(xmin + (w*width))\n",
    "        ymax = int(ymin + (h*height))\n",
    "        \n",
    "        cropped = im0[ymin:ymax, xmin:xmax]\n",
    "        \n",
    "        return cropped\n",
    "    \n",
    "    def rotate(self, img_plate):\n",
    "    \n",
    "        img_before = img_plate.copy()\n",
    "        img_before0 = img_plate.copy()\n",
    "\n",
    "        (h, w, d) = img_before0.shape \n",
    "        img_gray = cv2.cvtColor(img_before, cv2.COLOR_BGR2GRAY)\n",
    "        img_edges = cv2.Canny(img_gray, 100, 200, apertureSize=3)\n",
    "                \n",
    "        lines = cv2.HoughLinesP(img_edges, 1, math.pi / 180.0, 100, minLineLength=int(h/2), maxLineGap=10)\n",
    "        # print(type(lines))\n",
    "        angles = []\n",
    "        if lines is None:\n",
    "            angles = [0]\n",
    "        else:\n",
    "            for [[x1, y1, x2, y2]] in lines:\n",
    "                cv2.line(img_before, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "                angle = math.degrees(math.atan2(y2 - y1, x2 - x1)) #float\n",
    "                angles.append(angle)\n",
    "\n",
    "        if angles is None:\n",
    "            median_angle = float(1)\n",
    "        else:\n",
    "            median_angle = np.median(angles)\n",
    "        center = (w // 2, h // 2) #tuple\n",
    "        \n",
    "        M = cv2.getRotationMatrix2D(center, median_angle, 1.2) #numpy.ndarray\n",
    "        \n",
    "        # print(f\"Angle is {median_angle:.04f}\")\n",
    "        \n",
    "        rotated = cv2.warpAffine(img_before0, M, (w, h))\n",
    "        \n",
    "        return rotated\n",
    "\n",
    "    def crop_img(self, img_rotate):\n",
    "        img_rotated = img_rotate.copy()\n",
    "        h1, w1, d1 = img_rotated.shape\n",
    "        img_crop_on = img_rotated[0:h1//2, 0:w1]\n",
    "        img_crop_under= img_rotated[h1//2:h1, 0//2:w1]\n",
    "        \n",
    "        return img_crop_on, img_crop_under\n",
    "\n",
    "    \n",
    "    def show_img(self, crop_characters):\n",
    "        fig = plt.figure(figsize=(5,4))\n",
    "        grid = gridspec.GridSpec(ncols=len(crop_characters),nrows=1,figure=fig)\n",
    "\n",
    "        for i in range(len(crop_characters)):\n",
    "            fig.add_subplot(grid[i])\n",
    "            plt.axis(False)\n",
    "            cv2.imshow(\"c\",crop_characters[i])\n",
    "            cv2.waitKey()\n",
    "            # plt.imshow(crop_characters[i],cmap=\"gray\")  \n",
    "            \n",
    "    def detect(self, save_img=False):\n",
    "        crop_characters = None\n",
    "        plate_num = None\n",
    "        source, weights, view_img, save_txt, imgsz = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size\n",
    "        webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "            ('rtsp://', 'rtmp://', 'http://'))\n",
    "\n",
    "        # Directories\n",
    "        save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
    "        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "        # Initialize\n",
    "        set_logging()\n",
    "        device = select_device(opt.device)\n",
    "        half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "        # Load model\n",
    "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "        stride = int(model.stride.max())  # model stride\n",
    "        imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "        if half:\n",
    "            model.half()  # to FP16\n",
    "\n",
    "        # Second-stage classifier\n",
    "        classify = False\n",
    "        if classify:\n",
    "            modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "            modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
    "\n",
    "        # Set Dataloaderfilename\n",
    "        vid_path, vid_writer = None, None\n",
    "        if webcam:\n",
    "            view_img = check_imshow()\n",
    "            cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "            dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "        else:\n",
    "            save_img = True\n",
    "            dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "        # Get names and colors\n",
    "        names = model.module.names if hasattr(model, 'module') else model.names\n",
    "        colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "\n",
    "        # Run inference\n",
    "        if device.type != 'cpu':\n",
    "            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "        t0 = time.time()\n",
    "        for path, img, im0s, vid_cap in dataset:\n",
    "            img = torch.from_numpy(img).to(device)\n",
    "            img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "            if img.ndimension() == 3:\n",
    "                img = img.unsqueeze(0)\n",
    "\n",
    "            # Inference\n",
    "            t1 = time_synchronized()\n",
    "            pred = model(img, augment=opt.augment)[0]\n",
    "\n",
    "            # Apply NMS\n",
    "            pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "            t2 = time_synchronized()\n",
    "\n",
    "            # Apply Classifier\n",
    "            if classify:\n",
    "                pred = apply_classifier(pred, modelc, img, im0s)\n",
    "\n",
    "            # Process detections\n",
    "            for i, det in enumerate(pred):  # detections per image\n",
    "                if webcam:  # batch_size >= 1\n",
    "                    p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count\n",
    "                else:\n",
    "                    p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "\n",
    "                p = Path(p)  # to Path\n",
    "                save_path = str(save_dir / p.name)  # img.jpg\n",
    "                txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
    "                s += '%gx%g ' % img.shape[2:]  # print string\n",
    "                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "                if len(det):\n",
    "                    # Rescale boxes from img_size to im0 size\n",
    "                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "                    \n",
    "                    # Print results\n",
    "                    for c in det[:, -1].unique():\n",
    "                        n = (det[:, -1] == c).sum()  # detections per class\n",
    "                        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                    # Write results\n",
    "                    for *xyxy, conf, cls in reversed(det):\n",
    "                        if save_txt:  # Write to file\n",
    "                            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                            \n",
    "                            x = xywh[0]\n",
    "                            y = xywh[1]\n",
    "                            w = xywh[2]\n",
    "                            h = xywh[3]\n",
    "\n",
    "                            if save_img or view_img:  # Add bbox to image\n",
    "                                label = f'{names[int(cls)]} {conf:.2f}'\n",
    "                                plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "                            \n",
    "                            # img_plate = self.img_plate(xywh, im0)\n",
    "                            try:\n",
    "                                img_plate = self.img_plate(xywh, im0)\n",
    "                            except:\n",
    "                                pass\n",
    "                            \n",
    "                            try:\n",
    "                                crop_characters, plate_num = Recognition(opt01).recognition(img_plate)\n",
    "                            except:\n",
    "                                pass\n",
    "                            if (crop_characters is None) | (plate_num is None):\n",
    "                                continue\n",
    "                            \n",
    "                            # try:\n",
    "                            #     img_rotate = self.rotate(img_plate)\n",
    "                            # except:\n",
    "                            #     pass    \n",
    "                            \n",
    "                            # try:\n",
    "                            #     img_crop_on, img_crop_under = self.crop_img(img_rotate)\n",
    "                            # except:\n",
    "                            #     pass\n",
    "\n",
    "                            # try:\n",
    "                            #     crop_characters_on, plate_num_on = Recognition(opt01).recognition(img_crop_on)\n",
    "                            #     crop_characters_under, plate_num_under = Recognition(opt01).recognition(img_crop_under)\n",
    "                            #     crop_characters = crop_characters_on + crop_characters_under\n",
    "                            #     plate_num = plate_num_on + plate_num_under\n",
    "                            # except:\n",
    "                            #     pass\n",
    "                            \n",
    "                        \n",
    "                # Print time (inference + NMS)\n",
    "                print(f'{s}Done. ({t2 - t1:.3f}s)')\n",
    "\n",
    "                # Stream results\n",
    "                # if view_img:\n",
    "                    # cv2.imshow(str(p), im0)\n",
    "                    # cv2_imshow(im0)\n",
    "                    # cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "                # Save results (image with detections)\n",
    "                if save_img:\n",
    "                    if dataset.mode == 'image':\n",
    "                        cv2.imwrite(save_path, im0)\n",
    "                        \n",
    "                    else:  # 'video'\n",
    "                        if vid_path != save_path:  # new video\n",
    "                            vid_path = save_path\n",
    "                            if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                                vid_writer.release()  # release previous video writer\n",
    "\n",
    "                            fourcc = 'mp4v'  # output video codec\n",
    "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                            vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))\n",
    "                        vid_writer.write(im0)\n",
    "                        # cv2_imshow(im0)\n",
    "                \n",
    "        return crop_characters, plate_num\n",
    "        # return img_crop_on, img_crop_under, plate_num\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "Fok2X8UNmvy1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 03dddcf torch 1.8.0 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /media/tund/HDD/DL/LPR_yolov5/yolov5/data/images/107546983_603527793906914_3866777964214781071_n.jpg: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 03dddcf torch 1.8.0 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640x480 1 License_Plate, Done. (0.463s)\n",
      "9\n",
      "\u001b[31mNumber Plate :\u001b[0m L\n",
      "\f",
      "\f",
      "\f",
      "1\n",
      "\f",
      "9\n",
      "\f",
      "2\n",
      "\f",
      "Q\n",
      "\f",
      "7\n",
      "\f",
      "H\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Detect Plate\n",
    "\n",
    "crop_characters, plate_num = detectC(opt).detect()\n",
    "# plate_num = plate_num.replace(\"\\n\", \"\")\n",
    "print(len(crop_characters))\n",
    "# if not (crop_characters is None):\n",
    "#     detectC(opt).show_img(crop_characters)\n",
    "print(colored(\"Number Plate :\", \"red\"), plate_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56-AumuS4LVG"
   },
   "source": [
    "***END***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# im = \"data/images/107546983_603527793906914_3866777964214781071_n.jpg\"\n",
    "# im = cv2.imread(im)\n",
    "# cv2.imshow(\"a\", im)\n",
    "\n",
    "# if cv2.waitKey(0) and 0xFF==ord('q'):\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dPfbqtzOmNEc"
   },
   "outputs": [],
   "source": [
    "#@title Search file\n",
    "\n",
    "for rot, a, file in os.walk(\"/content/yolov5/runs/detect/\"):\n",
    "    for f in file:\n",
    "        if f.endswith(\".mp4\"):\n",
    "            print(os.path.abspath(f))\n",
    "            print(rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7mgHMSvuz1t2"
   },
   "outputs": [],
   "source": [
    "#@title Resize IMG\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# path_rs = '/content/yolov5/runs/detect/exp2/'\n",
    "# path_rs0 = '/content/yolov5/data/img_plate/'\n",
    "width_rs = 96\n",
    "height_rs = 96\n",
    "\n",
    "dim_rs = (width_rs, height_rs)\n",
    "ratio = width_rs/float(height_rs)\n",
    "\n",
    "def resize_plate(img_rs, ratio = ratio):\n",
    "    height, width, c_rs = img_rs.shape\n",
    "    aspect = width/float(height)\n",
    "\n",
    "    if aspect > ratio:\n",
    "        width_new = int(ratio * height)\n",
    "        offset = int((width-width_new)/2)\n",
    "        cropped = img_rs[0:height, offset:width]\n",
    "    else:\n",
    "        height_new = int(width/ratio)\n",
    "        cropped = img_rs[0:height_new, 0:width]\n",
    "\n",
    "    return cropped\n",
    "\n",
    "for filename in os.listdir(path_rs):\n",
    "    try:\n",
    "        img_rs = cv2.imread(path_rs + filename, cv2.IMREAD_UNCHANGED)\n",
    "        # print(path_rs + filename)\n",
    "        cropped = resize_plate(img_rs, ratio)\n",
    "        resized = cv2.resize(cropped, dim_rs, cv2.INTER_AREA)\n",
    "        cv2.imwrite(path_rs0 + filename, resized)    \n",
    "        # cv2_imshow(resized)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "yolov5_LPR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
